{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an attempt to make this notebook as organized (re: readable) as possible, I will explain its structure here. \n",
    "## The first cell is dedicated to initializing all dependencies used. \n",
    "## Each cell after is then dedicated to defining each function in the module\n",
    "### Finally, the last cell is this project's driver. That is where we will be putting the pieces of the puzzle together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports. 'Requests' for https requests. 'BeautifulSoup' for html scraping. 'Pandas' for data analysis. \n",
    "# 'sklearn' for similarity functions, such as word counter and cosine similarity. 'gensim' for Doc2Vec.\n",
    "# 'nltk' for pre-processing main text. 're' for regex. 'scipy' for spacial cosine. \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import doc2vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from scipy import spatial\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "# initializes training of doc2vec model. \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "# This can get initialized up here, as it will be constant throughout. \n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus class. This will hold all the article objects in our corpus and allow us to execute some corpus-wide methods.\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, main_article):\n",
    "        self.main_article = WikiArticle(main_article)\n",
    "        self.articles = []\n",
    "        self.corpus_stopwords = []\n",
    "        \n",
    "    def similarity_analysis(self):\n",
    "        \"\"\"Function: Analyzes all corpus against corpus main article and returns results in pandas dataframe.\n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           None\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns pandas dataframe with analysis results\"\"\"\n",
    "        \n",
    "        ## Pre-process main article\n",
    "        self.main_article.get_secondary_titles()\n",
    "        self.main_article.get_main_text()\n",
    "        self.main_article.get_word_frequency()\n",
    "        \n",
    "        \n",
    "        ## Initialize Pandas Dataframe to store results\n",
    "        index = []\n",
    "        for article in self.articles:\n",
    "            index.append(article.main_title)\n",
    "\n",
    "        columns = [\"Main Title Tier\", \"Secondary Title Tier\", \"Main Analysis\"]\n",
    "        results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "        ## Actually do the analysis\n",
    "        for article in self.articles:\n",
    "            results.loc[article.main_title] = self.main_article.similarity_analysis(article)\n",
    "        \n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def new_article(self, url):\n",
    "        \"\"\"Function: creates and stores a new article object. \n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           (Wiki) Article url\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns index of new article.\"\"\"\n",
    "        self.articles.append(WikiArticle(url))\n",
    "        return len(self.articles) - 1\n",
    "    \n",
    "    def filter_corpus_by_frequency(self, *args):\n",
    "        \"\"\"Function: Finds a variable amount (default 3) of the most frequent words in the corpus. \n",
    "           These words are then removed from all of the article.word_frequency[] dictionaries. \n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           (Optional) <int> Number of stop words to get. Default is 3. \n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           No return. Filters most frequent words in corpus out of all articles word_frequency dictionaries.\"\"\"\n",
    "\n",
    "        \n",
    "        ## Lets loop through the articles in the corpus, get each articles word_frequency count, and merge them into a common dictionary.\n",
    "        # ...this will be fun\n",
    "        total_frequency = {}\n",
    "        for article in self.articles:\n",
    "            total_frequency = mergeDict(total_frequency, article.get_word_frequency())\n",
    "        total_frequency = Counter(total_frequency)\n",
    "        ## Okay now that's done, let's get the most frequently found words in the corpus (aka our new corpus stop words).\n",
    "        \n",
    "        # Check if optional paramater was passed. \n",
    "        if(len(args) == 1):\n",
    "            count = args[0]\n",
    "        else:\n",
    "            count = 3\n",
    "\n",
    "        # Get 3 most frequently found words and store them in corpus_stopwords list. \n",
    "        self.corpus_stopwords = [item[0] for item in total_frequency.most_common(count)]\n",
    "        \n",
    "        # Loop through all articles in corpus, filtering out the newly obtained corpus stop words.\n",
    "        for article in self.articles:\n",
    "            article.filter_corpus_stopwords(self.corpus_stopwords)\n",
    "        \n",
    "        \n",
    "# Class agnostic function to help merging word_frequency dicts\n",
    "def mergeDict(dict1, dict2):\n",
    "    ''' Merge dictionaries and keep values of common keys in list'''\n",
    "    dict3 = {**dict1, **dict2}\n",
    "    for key, value in dict3.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            dict3[key] = (value + dict1[key])\n",
    "    return Counter(dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikiArticle class. Named 'wikiArticle' for lack of inspiration. Will hold all relevant data on an article. \n",
    "\n",
    "class WikiArticle:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = BeautifulSoup(requests.get(self.url).text, \"html\")\n",
    "        self.main_title = self.soup.find_all(\"h1\")[0].get_text()\n",
    "        self.secondary_titles = \"\"\n",
    "        self.main_text = \"\"\n",
    "        self.word_frequency = {}\n",
    "        \n",
    "    \n",
    "    def similarity_analysis(self, article):\n",
    "        \"\"\"\n",
    "        Function: To be used in Corpus class to perform analysis between main article and comparison article\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Article to be analyzed against main article.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns Pandas series to be used in forming a dataframe result.\"\"\"\n",
    "        \n",
    "        ## Check main title tier\n",
    "        if is_over_threshold(jaccard_analysis(self.main_title, article.main_title)):\n",
    "            main_title_tier = True\n",
    "        else:\n",
    "            return pd.Series({'Main Title Tier': False, 'Secondary Title Tier': None, 'Main Analysis': None})\n",
    "        \n",
    "        ## Check sub title tier\n",
    "        article.get_secondary_titles()\n",
    "        if is_over_threshold(jaccard_analysis(self.secondary_titles, article.secondary_titles)):\n",
    "            secondary_title_tier = True\n",
    "        else:\n",
    "            return pd.Series({'Main Title Tier': True, 'Secondary Title Tier': False, 'Main Analysis': None})\n",
    "        \n",
    "        ## Since we got this far we should get the main text and wrod count of the article\n",
    "        article.get_main_text()\n",
    "        article.get_word_frequency()\n",
    "        \n",
    "        ## And now perform the analysis \n",
    "        analysis = cosine_analysis(self.word_frequency, article.word_frequency)\n",
    "        return pd.Series({'Main Title Tier': True, 'Secondary Title Tier': True, 'Main Analysis':analysis})\n",
    "        \n",
    "            \n",
    "    \n",
    "    def get_secondary_titles(self):\n",
    "        # Check length to make sure secondary_titles list hasn't already been filled. Don't want duplicate data messing us up. \n",
    "        if(len(self.secondary_titles) == 0):\n",
    "            for secondary_title in self.soup.find_all(\"h2\"):\n",
    "                self.secondary_titles += \" \" + secondary_title.get_text()\n",
    "                \n",
    "    def get_main_text(self):\n",
    "        \"\"\"\n",
    "        Function: self.main_text set to <string> pre-processed main text of article.\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no parameters.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns nothing.\"\"\"\n",
    "        \n",
    "        # Gets text from the article\n",
    "        paragraphs = self.soup.find_all(\"p\")\n",
    "        article_text = \"\"\n",
    "        for p in paragraphs:\n",
    "            article_text = article_text + \" \" + p.text\n",
    "        \n",
    "        # Prepares text for analysis.\n",
    "        self.main_text = self.pre_process(article_text)\n",
    "    \n",
    "    def get_word_frequency(self):\n",
    "        \"\"\" \n",
    "        Function: gets word frequencies for article and stores in self.word_frequency dictionary. \n",
    "        ===========================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no paramaters\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns word frequency. Result is stored in self.word_frequency\"\"\"\n",
    "        \n",
    "        self.word_frequency = Counter(self.main_text)\n",
    "        return self.word_frequency\n",
    "        \n",
    "        \n",
    "    def filter_corpus_stopwords(self, corpus_stop_words):\n",
    "        \"\"\" \n",
    "        Function: removes all occurences of the most frequent words in the corpus from self.word_frequency. This function should only be called from within a corpus class method.\n",
    "        ===========================================\n",
    "        Parameters\n",
    "        ----------\n",
    "        <list> corpus stop words\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        No return. self.word_frequency\"\"\"\n",
    "        \n",
    "        filtered_text = Counter({})\n",
    "        for k, v in self.word_frequency.items():\n",
    "            if not k in corpus_stop_words:\n",
    "                filtered_text[k] = v\n",
    "                \n",
    "        self.word_frequency = filtered_text\n",
    "    \n",
    "    \n",
    "    def pre_process(self, text):\n",
    "        \"\"\"\n",
    "        Function: pre-processes text to prepare for analysis. \n",
    "        =====================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes <string> text to be pre-processed.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns <dict> Doc2Vec of pre-processed text.\"\"\"\n",
    "        \n",
    "        # Cleaing the text\n",
    "        processed_article = text.lower()\n",
    "        \n",
    "        # Preparing the dataset\n",
    "        all_words = word_tokenize(processed_article)\n",
    "        processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "        processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "        \n",
    "        # Removing Stop Words\n",
    "        processed_text = []\n",
    "        for w in all_words:\n",
    "            if not w in stopwords.words('english') and not w in string.punctuation:\n",
    "                processed_text.append(w)\n",
    "        \n",
    "        return processed_text\n",
    "\n",
    "    \n",
    "    def get_related(self):\n",
    "        \"\"\" Function: Get list of related articles\n",
    "            =====================================================\n",
    "                Parameters\n",
    "                ----------\n",
    "                This function takes no paramater \n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                This function returns the array of wikiArticle objects from listed related articles. \n",
    "               \n",
    "               \"\"\"\n",
    "        related_list = self.soup.find(id=\"See_also\").parent.find_next('ul').findChildren('li')\n",
    "        \n",
    "        articles = []\n",
    "        for item in related_list:\n",
    "            link = item.findChild('a')\n",
    "            articles.append(WikiArticle(\"https://en.wikipedia.org\"+link.get('href')))\n",
    "        \n",
    "        self.related = articles\n",
    "        return self.related\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_analysis(article_one, article_two):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       Right now this function takes two strings as its parameters (article_one, article_two). In the future, it should take \n",
    "       WikiArticle instances to allow multiple sub-headers to be analyzed together. \n",
    "       \n",
    "       Returns\n",
    "       --------\n",
    "       Jaccard Similarity Percentage.\"\"\"\n",
    "    \n",
    "    a = set(article_one.split(\" \"))\n",
    "    b = set(article_two.split(\" \"))\n",
    "    comparison = a.intersection(b)\n",
    "    return float(len(comparison)) / (len(a) + len(b) - len(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_analysis(article_one_frequency, article_two_frequency):\n",
    "\n",
    "    # convert to word-vectors\n",
    "    words  = list(article_one_frequency.keys() | article_two_frequency.keys())\n",
    "    a_vect = [article_one_frequency.get(word, 0) for word in words]        \n",
    "    b_vect = [article_two_frequency.get(word, 0) for word in words]       \n",
    "\n",
    "    # find cosine\n",
    "    len_a  = sum(av*av for av in a_vect) ** 0.5             \n",
    "    len_b  = sum(bv*bv for bv in b_vect) ** 0.5             \n",
    "    dot    = sum(av*bv for av,bv in zip(a_vect, b_vect))    \n",
    "    cosine = dot / (len_a * len_b)\n",
    "    \n",
    "    # return cosine\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_over_threshold(similarity, *args):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       similarity (float): similarity value that will be checked against threshold.\n",
    "       threshold (float): Optional paramater to provide value for threshold. Must be passed as \"threshold = (value)\". Default is 50.\n",
    "    \n",
    "       Returns\n",
    "       ----------\n",
    "       Boolean value. True if threshold limit is met or exceeded, else False.\"\"\"\n",
    "    \n",
    "    if(len(args) == 1):\n",
    "        threshold = args[0]\n",
    "    else:\n",
    "        threshold = .1\n",
    "    return (similarity >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: anaylsis results for main title were 0.0\n",
      "TEST: anaylsis results for main title were 0.0\n",
      "TEST: anaylsis results for main title were 0.0\n",
      "TEST: anaylsis results for main title were 0.0\n",
      "TEST: anaylsis results for main title were 0.0\n",
      "TEST: anaylsis results for main title were 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Main Title Tier</th>\n",
       "      <th>Secondary Title Tier</th>\n",
       "      <th>Main Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>List of IBM products</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.494956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Amdahl Corporation</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Watson (computer)</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Minicomputer</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Hewlett-Packard</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Data General Nova</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>History of hard disk drives</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Main Title Tier Secondary Title Tier Main Analysis\n",
       "List of IBM products                   True                 True      0.494956\n",
       "Amdahl Corporation                    False                 None          None\n",
       "Watson (computer)                     False                 None          None\n",
       "Minicomputer                          False                 None          None\n",
       "Hewlett-Packard                       False                 None          None\n",
       "Data General Nova                     False                 None          None\n",
       "History of hard disk drives           False                 None          None"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Driver ###\n",
    "##          ##\n",
    "# ========== #\n",
    "\n",
    "corpus = Corpus(\"https://en.wikipedia.org/wiki/IBM_mainframe\")\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/List_of_IBM_products\")\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/Amdahl_Corporation\")\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/Watson_(computer)\")\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/Minicomputer\")\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/Hewlett-Packard\")\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/Data_General_Nova\")\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/History_of_hard_disk_drives\")\n",
    "\n",
    "corpus.similarity_analysis()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Scratch Work ###\n",
    "##                ##\n",
    "# ================ #\n",
    "\n",
    "corpus.articles[0].get_main_text()\n",
    "\n",
    "corpus.filter_corpus_by_frequency(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ibm', \"'s\", 'company', 'computer', '``']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.corpus_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
