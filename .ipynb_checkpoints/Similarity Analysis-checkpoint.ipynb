{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an attempt to make this notebook as organized (re: readable) as possible, I will explain its structure here. \n",
    "## The first cell is dedicated to initializing all dependencies used. \n",
    "## Each cell after is then dedicated to defining each function in the module\n",
    "### Finally, the last cell is this project's driver. That is where we will be putting the pieces of the puzzle together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports. 'Requests' for https requests. 'BeautifulSoup' for html scraping. 'Pandas' for data analysis. \n",
    "# 'sklearn' for similarity functions, such as word counter and cosine similarity. 'gensim' for Doc2Vec.\n",
    "# 'nltk' for pre-processing main text. 're' for regex. 'scipy' for spacial cosine. \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import doc2vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy import spatial\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "# initializes training of doc2vec model. \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "# This can get initialized up here, as it will be constant throughout. \n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus class. This will hold all the article objects in our corpus and allow us to execute some corpus-wide methods.\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        self.articles = []\n",
    "        self.current_article_ndx = 0\n",
    "        self.corpus_stopwords = []\n",
    "    \n",
    "    def new_article(self, url):\n",
    "        \"\"\"Function: creates and stores a new article object. \n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           (Wiki) Article url\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns index of new article.\"\"\"\n",
    "        self.articles.append(WikiArticle(url))\n",
    "        return len(self.articles) - 1\n",
    "    \n",
    "    def filter_corpus_by_frequency(self, *args):\n",
    "        \"\"\"Function: Finds a variable amount (default 3) of the most frequent words in the corpus. \n",
    "           These words are then removed from all of the article.word_frequency[] dictionaries. \n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           (Optional) <int> Number of stop words to get. Default is 3. \n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           No return. Filters most frequent words in corpus out of all articles word_frequency dictionaries.\"\"\"\n",
    "\n",
    "        \n",
    "        ## Lets loop through the articles in the corpus, get each articles word_frequency count, and merge them into a common dictionary.\n",
    "        # ...this will be fun\n",
    "        total_frequency = {}\n",
    "        for article in self.articles:\n",
    "            total_frequency = mergeDict(total_frequency, article.get_word_frequency())\n",
    "        total_frequency = Counter(total_frequency)\n",
    "        ## Okay now that's done, let's get the most frequently found words in the corpus (aka our new corpus stop words).\n",
    "        \n",
    "        # Check if optional paramater was passed. \n",
    "        if(len(args) == 1):\n",
    "            count = args[0]\n",
    "        else:\n",
    "            count = 3\n",
    "        \n",
    "        print(type(total_frequency))\n",
    "        # Get 3 most frequently found words and store them in corpus_stopwords list. \n",
    "        print(total_frequency.most_common(count))\n",
    "        \n",
    "        # Loop through all articles in corpus, filtering out the newly obtained corpus stop words.\n",
    "        for article in self.articles:\n",
    "            article.filter_corpus_stopwords(self.corpus_stopwords)\n",
    "        \n",
    "        \n",
    "# Class agnostic function to help merging word_frequency dicts\n",
    "def mergeDict(dict1, dict2):\n",
    "    ''' Merge dictionaries and keep values of common keys in list'''\n",
    "    dict3 = {**dict1, **dict2}\n",
    "    for key, value in dict3.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            dict3[key] = [value + dict1[key]]\n",
    "    return Counter(dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikiArticle class. Named 'wikiArticle' for lack of inspiration. Will hold all relevant data on an article. \n",
    "\n",
    "class WikiArticle:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = BeautifulSoup(requests.get(self.url).text, \"html\")\n",
    "        self.main_title = self.soup.find_all(\"h1\")[0].get_text()\n",
    "        self.secondary_titles = \"\"\n",
    "        self.main_text = \"\"\n",
    "        self.word_frequency = {}\n",
    "        \n",
    "    def get_secondary_titles(self):\n",
    "        # Check length to make sure secondary_titles list hasn't already been filled. Don't want duplicate data messing us up. \n",
    "        if(len(self.secondary_titles) == 0):\n",
    "            for secondary_title in self.soup.find_all(\"h2\"):\n",
    "                self.secondary_titles += \" \" + secondary_title.get_text()\n",
    "                \n",
    "    def get_main_text(self):\n",
    "        \"\"\"\n",
    "        Function: self.main_text set to <string> pre-processed main text of article.\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no parameters.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns nothing.\"\"\"\n",
    "        \n",
    "        # Gets text from the article\n",
    "        paragraphs = self.soup.find_all(\"p\")\n",
    "        article_text = \"\"\n",
    "        for p in paragraphs:\n",
    "            article_text = article_text + \" \" + p.text\n",
    "        \n",
    "        # Prepares text for analysis.\n",
    "        self.main_text = self.pre_process(article_text)\n",
    "    \n",
    "    def get_word_frequency(self):\n",
    "        \"\"\" \n",
    "        Function: gets word frequencies for article and stores in self.word_frequency dictionary. \n",
    "        ===========================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no paramaters\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns word frequency. Result is stored in self.word_frequency\"\"\"\n",
    "        \n",
    "        self.word_frequency = Counter(self.main_text)\n",
    "        return self.word_frequency\n",
    "        \n",
    "        \n",
    "    def filter_corpus_stopwords(self, corpus_stop_words):\n",
    "        \"\"\" \n",
    "        Function: removes all occurences of the most frequent words in the corpus from self.word_frequency. This function should only be called from within a corpus class method.\n",
    "        ===========================================\n",
    "        Parameters\n",
    "        ----------\n",
    "        <list> corpus stop words\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        No return. self.word_frequency\"\"\"\n",
    "        \n",
    "        filtered_text = Counter({})\n",
    "        for k, v in self.word_frequency:\n",
    "            if not k in corpus_stop_words:\n",
    "                filtered_text[k] = v\n",
    "                \n",
    "        self.word_frequency = filtered_text\n",
    "    \n",
    "    \n",
    "    def pre_process(self, text):\n",
    "        \"\"\"\n",
    "        Function: pre-processes text to prepare for analysis. \n",
    "        =====================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes <string> text to be pre-processed.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns <dict> Doc2Vec of pre-processed text.\"\"\"\n",
    "        \n",
    "        # Cleaing the text\n",
    "        processed_article = text.lower()\n",
    "        \n",
    "        # Preparing the dataset\n",
    "        all_words = word_tokenize(processed_article)\n",
    "        processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "        processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "        \n",
    "        # Removing Stop Words\n",
    "        processed_text = []\n",
    "        for w in all_words:\n",
    "            if not w in stopwords.words('english'):\n",
    "                processed_text.append(w)\n",
    "        \n",
    "        return processed_text\n",
    "\n",
    "    \n",
    "    def get_related(self):\n",
    "        \"\"\" Function: Get list of related articles\n",
    "            =====================================================\n",
    "                Parameters\n",
    "                ----------\n",
    "                This function takes no paramater \n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                This function returns the array of wikiArticle objects from listed related articles. \n",
    "               \n",
    "               \"\"\"\n",
    "        related_list = self.soup.find(id=\"See_also\").parent.find_next('ul').findChildren('li')\n",
    "        \n",
    "        articles = []\n",
    "        for item in related_list:\n",
    "            link = item.findChild('a')\n",
    "            articles.append(WikiArticle(\"https://en.wikipedia.org\"+link.get('href')))\n",
    "        \n",
    "        self.related = articles\n",
    "        return self.related\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_analysis(article_one, article_two):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       Right now this function takes two strings as its parameters (article_one, article_two). In the future, it should take \n",
    "       WikiArticle instances to allow multiple sub-headers to be analyzed together. \n",
    "       \n",
    "       Returns\n",
    "       --------\n",
    "       Jaccard Similarity Percentage.\"\"\"\n",
    "    \n",
    "    a = set(article_one.split(\" \"))\n",
    "    b = set(article_two.split(\" \"))\n",
    "    comparison = a.intersection(b)\n",
    "    return float(len(comparison)) / (len(a) + len(b) - len(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_analysis(text_one, text_two):\n",
    "    # count word frequency. \n",
    "    # We can't reuse the WikiArticle.get_word_frequency() method since we're dealing with strings and not article objects... \n",
    "    # ... that's okay cause this gives us more flexibility as to what we can perform cosine similarity analysis on.  \n",
    "    article_one_vals = Counter(text_one)\n",
    "    article_two_vals = Counter(text_two)\n",
    "\n",
    "    # convert to word-vectors\n",
    "    words  = list(article_one_vals.keys() | article_two_vals.keys())\n",
    "    a_vect = [article_one_vals.get(word, 0) for word in words]        \n",
    "    b_vect = [article_two_vals.get(word, 0) for word in words]       \n",
    "\n",
    "    # find cosine\n",
    "    len_a  = sum(av*av for av in a_vect) ** 0.5             \n",
    "    len_b  = sum(bv*bv for bv in b_vect) ** 0.5             \n",
    "    dot    = sum(av*bv for av,bv in zip(a_vect, b_vect))    \n",
    "    cosine = dot / (len_a * len_b)\n",
    "    \n",
    "    # return cosine\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_over_threshold(similarity, *args):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       similarity (float): similarity value that will be checked against threshold.\n",
    "       threshold (float): Optional paramater to provide value for threshold. Must be passed as \"threshold = (value)\". Default is 50.\n",
    "    \n",
    "       Returns\n",
    "       ----------\n",
    "       Boolean value. True if threshold limit is met or exceeded, else False.\"\"\"\n",
    "    \n",
    "    if(len(args) == 1):\n",
    "        threshold = args[0]\n",
    "    else:\n",
    "        threshold = 50\n",
    "    return (similarity >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is over threshold.\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "### Driver ###\n",
    "##          ##\n",
    "# ========== #\n",
    "\n",
    "corpus = Corpus()\n",
    "\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/IBM_mainframe\")\n",
    "corpus.new_article(\"https://en.wikipedia.org/wiki/History_of_IBM\")\n",
    "\n",
    "# Check if main title similarity is over threshold\n",
    "if(is_over_threshold(jaccard_analysis(corpus.articles[0].main_title, corpus.articles[1].main_title), 0.10)):\n",
    "    print(\"is over threshold.\")\n",
    "else:\n",
    "    print(\"is not over threshold\")\n",
    "\n",
    "print(jaccard_analysis(corpus.articles[0].main_title, corpus.articles[1].main_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Context\n",
      "<class 'collections.Counter'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-3a1b2bd36348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_main_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_corpus_by_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-c7b15c64bc2e>\u001b[0m in \u001b[0;36mfilter_corpus_by_frequency\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Get 3 most frequently found words and store them in corpus_stopwords list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_frequency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Loop through all articles in corpus, filtering out the newly obtained corpus stop words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/collections/__init__.py\u001b[0m in \u001b[0;36mmost_common\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_itemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_heapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_itemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/heapq.py\u001b[0m in \u001b[0;36mnlargest\u001b[0;34m(n, iterable, key)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "### Scratch Work ###\n",
    "##                ##\n",
    "# ================ #\n",
    "%xmode context\n",
    "\n",
    "corpus.articles[0].get_main_text()\n",
    "corpus.articles[1].get_main_text()\n",
    "\n",
    "corpus.filter_corpus_by_frequency(1)\n",
    "\n",
    "print(corpus.corpus_stopwords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
