{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an attempt to make this notebook as organized (re: readable) as possible, I will explain its structure here. \n",
    "## The first cell is dedicated to initializing all dependencies used. \n",
    "## Each cell after is then dedicated to defining each function in the module\n",
    "### Finally, the last cell is this project's driver. That is where we will be putting the pieces of the puzzle together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# Imports. 'Requests' for https requests. 'BeautifulSoup' for html scraping. 'Pandas' for data analysis. \n",
    "# 'sklearn' for similarity functions, such as word counter and cosine similarity. 'gensim' for Doc2Vec.\n",
    "# 'nltk' for pre-processing main text. 're' for regex. 'scipy' for spacial cosine. \n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import doc2vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from scipy import spatial\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "from os import path\n",
    "import argparse\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This can get initialized up here, as it will be constant throughout. \n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus class. This will hold all the article objects in our corpus and allow us to execute some corpus-wide methods.\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, main_article):\n",
    "        self.main_article = WikiArticle(main_article)\n",
    "        self.articles = []\n",
    "        self.corpus_stopwords = []\n",
    "        self.D2Vmodel = None\n",
    "        self.W2Vmodel = None\n",
    "        self.fill_type = \"\"\n",
    "        \n",
    "    def fill_corpus(self, size, mode):\n",
    "        \"\"\"Function: Fills corpus by getting related articles, starting with the main article and\n",
    "           using the other articles that are found until the corpus meets the set size parameter.  \n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Desired size of final corpus.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           No return. Corpus articles are stored in self.articles\"\"\"\n",
    "        if(mode == \"all_random\"):\n",
    "            while len(self.articles) < size:\n",
    "                art = WikiArticle(\"https://en.wikipedia.org/wiki/Special:Random\")\n",
    "                self.articles.append(art)\n",
    "                \n",
    "        if(mode == \"all_related\"):\n",
    "            # Start filling corpus with artiles related to main article\n",
    "            self.articles = self.main_article.get_related()\n",
    "\n",
    "            # Keep track of current article using article counter\n",
    "            article_counter = 0\n",
    "\n",
    "            while len(self.articles) < size:\n",
    "                try:\n",
    "                    related_articles = self.articles[article_counter].get_related()\n",
    "                    if(related_articles != False):\n",
    "                        self.articles.extend(self.articles[article_counter].get_related())\n",
    "                except:\n",
    "                    print(\"Error with getting related article for \" + self.articles[article_counter].main_title)\n",
    "                article_counter +=1\n",
    "                \n",
    "                \n",
    "        if(mode==\"fifty_fifty\"):\n",
    "            #Start filling corpus with articles related to main article\n",
    "            article_counter = 0\n",
    "            self.articles = self.main_article.get_related()\n",
    "            while len(self.articles) < size/2:\n",
    "                related_articles = self.articles[article_counter].get_related()\n",
    "                if(related_articles != False):\n",
    "                    self.articles.extend(self.articles[article_counter].get_related())\n",
    "                article_counter +=1\n",
    "                \n",
    "            while len(self.articles) < size:\n",
    "                art = WikiArticle(\"https://en.wikipedia.org/wiki/Special:Random\")\n",
    "                self.articles.append(art)\n",
    "        \n",
    "        self.fill_type = mode\n",
    "                   \n",
    "        \n",
    "    def similarity_analysis(self, is_smart, d2v_model, w2v_model, index2word_set):\n",
    "        \"\"\"Function: Analyzes all corpus against corpus main article and returns results in pandas dataframe.\n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           <boolean> Determines whether or not multi-tiered analysis is used. \n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns pandas dataframe with analysis results\"\"\"\n",
    "        \n",
    "        ## Pre-process main article\n",
    "        self.main_article.get_secondary_titles()\n",
    "        self.main_article.get_main_text()\n",
    "        self.main_article.get_word_frequency()\n",
    "        \n",
    "        ## Initialize Pandas Dataframe to store results\n",
    "        index = []\n",
    "        for article in self.articles:\n",
    "            if(article.main_title not in index):\n",
    "                index.append(article.main_title)\n",
    "\n",
    "        columns = [\"Main Title Tier\", \"Secondary Title Tier\", \"Main Analysis\"]\n",
    "        results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "        ## Actually do the analysis\n",
    "        ndx = 1 ## index for d2v tagging purposes\n",
    "        for article in self.articles:\n",
    "            results.loc[article.main_title] = self.main_article.similarity_analysis(article, is_smart, ndx, d2v_model, w2v_model, index2word_set)\n",
    "            ndx = ndx + 1\n",
    "        \n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def new_article(self, url):\n",
    "        \"\"\"Function: add a specific wiki article to the corpus.\n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           (Wiki) Article url\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns index of new article.\"\"\"\n",
    "        self.articles.append(WikiArticle(url))\n",
    "        return len(self.articles) - 1\n",
    "    \n",
    "    def filter_corpus_by_frequency(self, *args):\n",
    "        \"\"\"Function: Finds a variable amount (default 3) of the most frequent words in the corpus. \n",
    "           These words are then removed from all of the article.word_frequency[] dictionaries. \n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           (Optional) <int> Number of stop words to get. Default is 3. \n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           No return. Filters most frequent words in corpus out of all articles word_frequency dictionaries.\"\"\"\n",
    "\n",
    "        \n",
    "        ## Lets loop through the articles in the corpus, get each articles word_frequency count, and merge them into a common dictionary.\n",
    "        # ...this will be fun\n",
    "        total_frequency = {}\n",
    "        for article in self.articles:\n",
    "            total_frequency = mergeDict(total_frequency, article.get_word_frequency())\n",
    "        total_frequency = Counter(total_frequency)\n",
    "        ## Okay now that's done, let's get the most frequently found words in the corpus (aka our new corpus stop words).\n",
    "        \n",
    "        # Check if optional paramater was passed. \n",
    "        if(len(args) == 1):\n",
    "            count = args[0]\n",
    "        else:\n",
    "            count = 3\n",
    "\n",
    "        # Get 3 most frequently found words and store them in corpus_stopwords list. \n",
    "        self.corpus_stopwords = [item[0] for item in total_frequency.most_common(count)]\n",
    "        \n",
    "        # Loop through all articles in corpus, filtering out the newly obtained corpus stop words.\n",
    "        for article in self.articles:\n",
    "            article.filter_corpus_stopwords(self.corpus_stopwords)\n",
    "        \n",
    "        \n",
    "# Class agnostic function to help merging word_frequency dicts\n",
    "def mergeDict(dict1, dict2):\n",
    "    ''' Merge dictionaries and keep values of common keys in list'''\n",
    "    dict3 = {**dict1, **dict2}\n",
    "    for key, value in dict3.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            dict3[key] = (value + dict1[key])\n",
    "    return Counter(dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikiArticle class. Named 'wikiArticle' for lack of inspiration. Will hold all relevant data on an article. \n",
    "\n",
    "class WikiArticle:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = BeautifulSoup(requests.get(self.url).text, features=\"html.parser\")\n",
    "        self.main_title = self.soup.find_all(\"h1\")[0].get_text()\n",
    "        self.secondary_titles = \"\"\n",
    "        self.main_text = \"\"\n",
    "        self.word_frequency = {}\n",
    "        \n",
    "    \n",
    "    def similarity_analysis(self, article, is_smart, ndx, d2v_model, w2v_model, index2word_set):\n",
    "        \"\"\"\n",
    "        Function: To be used in Corpus class to perform analysis between main article and comparison article\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Article to be analyzed against main article.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns Pandas series to be used in forming a dataframe result.\"\"\"\n",
    "        ## Check main title tier\n",
    "        if(is_smart == True):\n",
    "            a_vec = d2v_model.infer_vector(self.pre_process(self.main_title))\n",
    "            b_vec = d2v_model.infer_vector(article.pre_process(article.main_title))\n",
    "            main_title_comparison = vec_cosine_analysis(a_vec, b_vec)\n",
    "            if is_over_threshold(main_title_comparison):\n",
    "                main_title_tier = True\n",
    "            else:\n",
    "                return pd.Series({'Main Title Tier': main_title_comparison, 'Secondary Title Tier': None, 'Main Analysis': None})\n",
    "\n",
    "            ## Check sub title tier\n",
    "            article.get_secondary_titles()\n",
    "            a_vec = d2v_model.infer_vector(self.pre_process(self.secondary_titles))\n",
    "            b_vec = d2v_model.infer_vector(article.pre_process(article.secondary_titles))\n",
    "            secondary_title_comparison = vec_cosine_analysis(a_vec, b_vec)\n",
    "            if is_over_threshold(secondary_title_comparison):\n",
    "                secondary_title_tier = True\n",
    "            else:\n",
    "                return pd.Series({'Main Title Tier': main_title_comparison, 'Secondary Title Tier': secondary_title_comparison, 'Main Analysis': None})\n",
    "        else: \n",
    "            main_title_comparison = None;\n",
    "            secondary_title_comparison = None;\n",
    "        \n",
    "        ## Since we got this far we should get the main text and wrod count of the article\n",
    "        article.get_main_text()\n",
    "        article.get_word_frequency()\n",
    "        \n",
    "        ## And now perform the main analysis \n",
    "        a_vec = d2v_model.infer_vector(self.main_text)\n",
    "        b_vec = d2v_model.infer_vector(article.main_text)\n",
    "        analysis = vec_cosine_analysis(a_vec, b_vec)\n",
    "        return pd.Series({'Main Title Tier': main_title_comparison, 'Secondary Title Tier': secondary_title_comparison, 'Main Analysis':analysis})\n",
    "        \n",
    "            \n",
    "    \n",
    "    def get_secondary_titles(self):\n",
    "        # Check length to make sure secondary_titles list hasn't already been filled. Don't want duplicate data messing us up. \n",
    "        if(len(self.secondary_titles) == 0):\n",
    "            for secondary_title in self.soup.find_all(\"h2\"):\n",
    "                self.secondary_titles += \" \" + secondary_title.get_text()\n",
    "    \n",
    "    def w2v_vector(self, text, model, index2word_set):\n",
    "        \"\"\"\n",
    "        Function: To get word2vec average vector of provided text\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Text to be word2vec'd\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Word2Vec average vector of provided text.\"\"\"\n",
    "        num_features = model.wv.vectors.shape[1];\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "\n",
    "        for word in text:\n",
    "            if word in index2word_set:\n",
    "                nwords = nwords+1\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "        if nwords>0:\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec\n",
    "        \n",
    "        \n",
    "                \n",
    "    def get_main_text(self):\n",
    "        \"\"\"\n",
    "        Function: self.main_text set to <string> pre-processed main text of article.\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no parameters.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns nothing.\"\"\"\n",
    "        \n",
    "        # Gets text from the article\n",
    "        paragraphs = self.soup.find_all(\"p\")\n",
    "        article_text = \"\"\n",
    "        for p in paragraphs:\n",
    "            article_text = article_text + \" \" + p.text\n",
    "        \n",
    "        # Prepares text for analysis.\n",
    "        self.main_text = self.pre_process(article_text)\n",
    "    \n",
    "    def get_word_frequency(self):\n",
    "        \"\"\" \n",
    "        Function: gets word frequencies for article and stores in self.word_frequency dictionary. \n",
    "        ===========================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no paramaters\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns word frequency. Result is stored in self.word_frequency\"\"\"\n",
    "        \n",
    "        self.word_frequency = Counter(self.main_text)\n",
    "        return self.word_frequency\n",
    "        \n",
    "        \n",
    "    def filter_corpus_stopwords(self, corpus_stop_words):\n",
    "        \"\"\" \n",
    "        Function: removes all occurences of the most frequent words in the corpus from self.word_frequency. This function should only be called from within a corpus class method.\n",
    "        ===========================================\n",
    "        Parameters\n",
    "        ----------\n",
    "        <list> corpus stop words\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        No return. self.word_frequency\"\"\"\n",
    "        \n",
    "        filtered_text = Counter({})\n",
    "        for k, v in self.word_frequency.items():\n",
    "            if not k in corpus_stop_words:\n",
    "                filtered_text[k] = v\n",
    "                \n",
    "        self.word_frequency = filtered_text\n",
    "    \n",
    "    \n",
    "    def pre_process(self, text):\n",
    "        \"\"\"\n",
    "        Function: pre-processes text to prepare for analysis. \n",
    "        =====================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes <string> text to be pre-processed.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns <dict> of pre-processed text.\"\"\"\n",
    "        \n",
    "        # Cleaing the text\n",
    "        processed_article = text.lower()\n",
    "        \n",
    "        # Preparing the dataset\n",
    "        all_words = word_tokenize(processed_article)\n",
    "        processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "        processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "        \n",
    "        # Removing Stop Words\n",
    "        processed_text = []\n",
    "        for w in all_words:\n",
    "            if not w in stopwords.words('english') and not w in string.punctuation:\n",
    "                processed_text.append(w)\n",
    "        \n",
    "        return processed_text\n",
    "\n",
    "    \n",
    "    def get_related(self):\n",
    "        \"\"\" Function: Get list of related articles\n",
    "            =====================================================\n",
    "                Parameters\n",
    "                ----------\n",
    "                This function takes no paramater \n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                If related articles exist, they are returned in a list. Else returns false. \n",
    "               \n",
    "               \"\"\"\n",
    "        if(self.soup.find(id=\"See_also\") is None):\n",
    "            return False\n",
    "        else:\n",
    "            related_list = self.soup.find(id=\"See_also\").parent.find_next('ul').findChildren('li')\n",
    "\n",
    "            articles = []\n",
    "            for item in related_list:\n",
    "                link = item.findChild('a')\n",
    "                articles.append(WikiArticle(\"https://en.wikipedia.org\"+link.get('href')))\n",
    "\n",
    "            return articles\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## d2v and w2v model building methods.\n",
    "\n",
    "def build_d2v_model(corpus):\n",
    "    \"\"\"Function: Builds d2v model for analysis. Model is loaded from file if it already exists.\n",
    "       ============================================================================\n",
    "       Parameters\n",
    "       ----------\n",
    "       Corpus to build model on (all_related preferably).\n",
    "\n",
    "       Returns\n",
    "       ----------\n",
    "       Builds, trains, and returns doc2vec model\"\"\"\n",
    "    if(path.exists(\"D2Vmodel\")):\n",
    "        D2Vmodel = doc2vec.Doc2Vec.load(\"D2Vmodel\")\n",
    "        return D2Vmodel\n",
    "\n",
    "    else:\n",
    "        training_corpus = []\n",
    "        corpus.main_article.get_main_text()\n",
    "        training_corpus.append(gensim.models.doc2vec.TaggedDocument(words=corpus.main_article.main_text, tags=[0]))\n",
    "        tag_counter = 0\n",
    "        for article in corpus.articles:\n",
    "            article.get_main_text()\n",
    "            training_corpus.append(gensim.models.doc2vec.TaggedDocument(words=article.main_text, tags=[tag_counter]))\n",
    "            tag_counter += 1\n",
    "\n",
    "        D2Vmodel = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=10)\n",
    "        D2Vmodel.build_vocab(training_corpus)\n",
    "        D2Vmodel.train(training_corpus, total_examples=D2Vmodel.corpus_count, epochs=20)\n",
    "        D2Vmodel.save(\"D2Vmodel\")\n",
    "        print (\"Doc2vec model was made with all_related corpus\")\n",
    "        return D2Vmodel\n",
    "        \n",
    "        \n",
    "def build_w2v_model(corpus):\n",
    "    \"\"\"Function: Builds w2v model for analysis. Model is loaded from file if it already exists.\n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Corpus to build model on (all_related preferably).\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Builds, trains, and returns word2vec model\"\"\"\n",
    "    if(path.exists(\"W2Vmodel\")):\n",
    "        w2v_model = Word2Vec.load(\"W2Vmodel\")\n",
    "        return w2v_model\n",
    "    \n",
    "    else: \n",
    "        training_corpus = []\n",
    "        corpus.main_article.get_main_text()\n",
    "        for word in corpus.main_article.main_text:\n",
    "            training_corpus.append(word)\n",
    "            \n",
    "        for article in corpus.articles:\n",
    "            for word in article.main_text:\n",
    "                training_corpus.append(word)\n",
    "                \n",
    "        w2v_model = Word2Vec(sentences=training_corpus, min_count = 10, size=300)\n",
    "        w2v_model.save(\"W2Vmodel\")\n",
    "        print(\"Word2Vec model was made with all_related corpus\")\n",
    "        return w2v_model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_analysis(article_one, article_two):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       Right now this function takes two strings as its parameters (article_one, article_two). In the future, it should take \n",
    "       WikiArticle instances to allow multiple sub-headers to be analyzed together. \n",
    "       \n",
    "       Returns\n",
    "       --------\n",
    "       Jaccard Similarity Percentage.\"\"\"\n",
    "    \n",
    "    a = set(article_one.split(\" \"))\n",
    "    b = set(article_two.split(\" \"))\n",
    "    comparison = a.intersection(b)\n",
    "    return float(len(comparison)) / (len(a) + len(b) - len(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_analysis(article_one_frequency, article_two_frequency):\n",
    "\n",
    "    # convert to word-vectors\n",
    "    words  = list(article_one_frequency.keys() | article_two_frequency.keys())\n",
    "    a_vect = [article_one_frequency.get(word, 0) for word in words]        \n",
    "    b_vect = [article_two_frequency.get(word, 0) for word in words]       \n",
    "\n",
    "    # find cosine\n",
    "    len_a  = sum(av*av for av in a_vect) ** 0.5             \n",
    "    len_b  = sum(bv*bv for bv in b_vect) ** 0.5             \n",
    "    dot    = sum(av*bv for av,bv in zip(a_vect, b_vect))    \n",
    "    cosine = dot / (len_a * len_b)\n",
    "    \n",
    "    # return cosine\n",
    "    return cosine\n",
    "\n",
    "def vec_cosine_analysis(a_vect, b_vect):\n",
    "    # find cosine\n",
    "    len_a  = sum(av*av for av in a_vect) ** 0.5             \n",
    "    len_b  = sum(bv*bv for bv in b_vect) ** 0.5             \n",
    "    dot    = sum(av*bv for av,bv in zip(a_vect, b_vect))    \n",
    "    cosine = dot / (len_a * len_b)\n",
    "    \n",
    "    # return cosine\n",
    "    return cosine\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_over_threshold(similarity, *args):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       similarity (float): similarity value that will be checked against threshold.\n",
    "       threshold (float): Optional paramater to provide value for threshold. Must be passed as \"threshold = (value)\". Default is 50.\n",
    "    \n",
    "       Returns\n",
    "       ----------\n",
    "       Boolean value. True if threshold limit is met or exceeded, else False.\"\"\"\n",
    "    \n",
    "    if(len(args) == 1):\n",
    "        threshold = args[0]\n",
    "    else:\n",
    "        threshold = .5\n",
    "    return (similarity >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/dannyobrien/Library/Jupyter/runtime/kernel-fe28090b-f899-4c35-b146-a2a7a0e74dfa.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "### Driver ###\n",
    "##          ##\n",
    "# ========== #/\n",
    "\n",
    "# Initiate corpus with article of focus\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## Receive args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"size\", type=int)\n",
    "    parser.add_argument(\"--seed\", type=int)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    wikiUrl = \"https://en.wikipedia.org/wiki/\"\n",
    "    \n",
    "    seeds = [\"IBM_mainframe\", \"Information_technology\", \"Algorithm\"]\n",
    "    if args.seed:\n",
    "        seed = args.seed\n",
    "    else:\n",
    "        seed = random.randrange(0,3,1)\n",
    "    \n",
    "    wikiUrl+= seeds[seed]\n",
    "    \n",
    "    ## Create and fill corpora \n",
    "    corpus_related = Corpus(wikiUrl)\n",
    "    corpus_random = Corpus(wikiUrl)\n",
    "    corpus_fifty_fifty = Corpus(wikiUrl)\n",
    "    corpus_related.fill_corpus(args.size, \"all_related\")\n",
    "    corpus_random.fill_corpus(args.size, \"all_random\")\n",
    "    corpus_fifty_fifty.fill_corpus(args.size, \"fifty_fifty\")\n",
    "    \n",
    "    ## Issue #1 in https://github.com/danielobrien3/HBC-Text-Similarity-Analysis\n",
    "    #corpus.filter_corpus_by_frequency()\n",
    "    \n",
    "    ## Loads model from file. The model is created if it does not yet exist. \n",
    "    d2v_model = build_d2v_model(corpus_related)\n",
    "    w2v_model = build_w2v_model(corpus_related)\n",
    "    index2word_set = set(w2v_model.wv.index2word)\n",
    "    \n",
    "    \n",
    "    ## Get (multitiered AND regular analysis) results of each corpus\n",
    "    \n",
    "    # related corpus\n",
    "    start = time.time()\n",
    "    smart_results_related = corpus_related.similarity_analysis(True, d2v_model, w2v_model, index2word_set)\n",
    "    end = time.time()\n",
    "    smart_results_related_time = end - start\n",
    "    \n",
    "    start = time.time()\n",
    "    dumb_results_related = corpus_related.similarity_analysis(False, d2v_model, w2v_model, index2word_set)\n",
    "    end = time.time()\n",
    "    dumb_results_related_time = end - start\n",
    "    \n",
    "    \n",
    "    # random corpus\n",
    "    start = time.time()\n",
    "    smart_results_random = corpus_random.similarity_analysis(True, d2v_model, w2v_model, index2word_set)\n",
    "    end = time.time()\n",
    "    smart_results_random_time = end - start\n",
    "    \n",
    "    start = time.time()\n",
    "    dumb_results_random = corpus_random.similarity_analysis(False, d2v_model, w2v_model, index2word_set)\n",
    "    end = time.time()\n",
    "    dumb_results_random_time = end - start\n",
    "    \n",
    "    # fifty fifty corpus\n",
    "    start = time.time()\n",
    "    smart_results_fifty = corpus_fifty_fifty.similarity_analysis(True, d2v_model, w2v_model, index2word_set)\n",
    "    end = time.time()\n",
    "    smart_results_fifty_time = end - start\n",
    "    \n",
    "    start = time.time()\n",
    "    dumb_results_fifty = corpus_fifty_fifty.similarity_analysis(False, d2v_model, w2v_model, index2word_set)\n",
    "    end = time.time()\n",
    "    dumb_results_fifty_time = end - start\n",
    "    \n",
    "    \n",
    "    ## Export results in csv files, storing time results first\n",
    "    # Creates the csv if it doesn't exist\n",
    "    if not(path.exists(\"time_results.csv\")):\n",
    "        with open('time_results.csv', 'w') as csvfile:\n",
    "            filewriter = csv.writer(csvfile)\n",
    "            filewriter.writerow(['Corpus Type', 'Multi-tiered', 'Size', 'Time elapsed (seconds)', 'Seed'])\n",
    "        \n",
    "    # Writes results to csv\n",
    "    with open('time_results.csv', 'a') as csvfile:\n",
    "        filewriter = csv.writer(csvfile)\n",
    "        filewriter.writerow(['related', 'TRUE', args.size, smart_results_related_time, seeds[seed]])\n",
    "        filewriter.writerow(['related', 'FALSE', args.size, dumb_results_related_time, seeds[seed]])\n",
    "        filewriter.writerow(['random', 'TRUE', args.size, smart_results_random_time, None])\n",
    "        filewriter.writerow(['random', 'FALSE', args.size, dumb_results_random_time, None])\n",
    "        filewriter.writerow(['fifty_fifty', 'TRUE', args.size, smart_results_fifty_time, seeds[seed]])\n",
    "        filewriter.writerow(['fifty_fifty', 'FALSE', args.size, dumb_results_fifty_time, seeds[seed]])\n",
    "    \n",
    "    # Create file names first.\n",
    "    smart_related_file = \"multitiered_\"  + str(args.size) + \"_related.csv\"\n",
    "    dumb_related_file = fileTwo = \"regular_\"  + str(args.size) + \"_related.csv\"\n",
    "    \n",
    "    smart_random_file = \"multitiered_\"  + str(args.size) + \"_random.csv\"\n",
    "    dumb_random_file = \"regular_\"  + str(args.size) + \"_random.csv\"\n",
    "    \n",
    "    smart_fifty_fifty_file = \"multitiered_\"  + str(args.size) + \"_fifty_fifty.csv\"\n",
    "    dumb_fifty_fifty_file = \"regular_\"  + str(args.size) + \"_fifty_fifty.csv\"\n",
    "    \n",
    "    smart_results_related.to_csv(smart_related_file)\n",
    "    dumb_results_related.to_csv(dumb_related_file)\n",
    "    \n",
    "    smart_results_random.to_csv(smart_random_file)\n",
    "    dumb_results_random.to_csv(dumb_random_file)\n",
    "    \n",
    "    smart_results_fifty.to_csv(smart_fifty_fifty_file)\n",
    "    dumb_results_fifty.to_csv(dumb_fifty_fifty_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Main Title Tier  \\\n",
      "List of IBM products                             0.891771   \n",
      "Amdahl Corporation                               0.883806   \n",
      "Midrange computer                                0.919839   \n",
      "Linux on IBM Z                                   0.870375   \n",
      "IBM Secure Service Container                     0.904897   \n",
      "...                                                   ...   \n",
      "Multi-processor system-on-chip                    0.87808   \n",
      "Manycore processor                               0.841748   \n",
      "CUDA                                             0.525753   \n",
      "Globally asynchronous locally synchronous         0.79046   \n",
      "Network architecture                                0.872   \n",
      "\n",
      "                                          Secondary Title Tier Main Analysis  \n",
      "List of IBM products                                  0.994863      0.719423  \n",
      "Amdahl Corporation                                    0.997554      0.580913  \n",
      "Midrange computer                                     0.997238      0.683922  \n",
      "Linux on IBM Z                                        0.995567      0.716243  \n",
      "IBM Secure Service Container                          0.996351      0.716948  \n",
      "...                                                        ...           ...  \n",
      "Multi-processor system-on-chip                        0.997858      0.350226  \n",
      "Manycore processor                                    0.994252      0.409978  \n",
      "CUDA                                                  0.995106      0.339453  \n",
      "Globally asynchronous locally synchronous              0.99298      0.296377  \n",
      "Network architecture                                  0.997387      0.263728  \n",
      "\n",
      "[354 rows x 3 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
