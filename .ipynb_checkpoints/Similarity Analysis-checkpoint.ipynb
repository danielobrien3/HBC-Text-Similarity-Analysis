{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an attempt to make this notebook as organized (re: readable) as possible, I will explain its structure here. \n",
    "## The first cell is dedicated to initializing all dependencies used. \n",
    "## Each cell after is then dedicated to defining each function in the module\n",
    "### Finally, the last cell is this project's driver. That is where we will be putting the pieces of the puzzle together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports. 'Requests' for https requests. 'BeautifulSoup' for html scraping. 'Pandas' for data analysis. \n",
    "# 'sklearn' for similarity functions, such as word counter and cosine similarity. 'gensim' for Word2Vec.\n",
    "# 'nltk' for pre-processing main text. 're' for regex. 'scipy' for spacial cosine. \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy import spatial\n",
    "\n",
    "# This can get initialized up here, as it will be constant throughout. \n",
    "count_vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikiArticle class. Named 'wikiArticle' for lack of inspiration. Will hold all relevant data on an article. \n",
    "\n",
    "class WikiArticle:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = BeautifulSoup(requests.get(self.url).text, \"html\")\n",
    "        self.main_title = self.soup.find_all(\"h1\")[0].get_text()\n",
    "        self.secondary_titles = \"\"\n",
    "        self.main_text = \"\"\n",
    "        \n",
    "    def get_secondary_titles(self):\n",
    "        # Check length to make sure secondary_titles list hasn't already been filled. Don't want duplicate data messing us up. \n",
    "        if(len(self.secondary_titles) == 0):\n",
    "            for secondary_title in self.soup.find_all(\"h2\"):\n",
    "                self.secondary_titles += \" \" + secondary_title.get_text()\n",
    "                \n",
    "    def get_main_text(self):\n",
    "        \"\"\"\n",
    "        Function: self.main_text set to <string> pre-processed main text of article.\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no parameters.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns nothing.\"\"\"\n",
    "        \n",
    "        # Gets text from the article\n",
    "        paragraphs = self.soup.find_all(\"p\")\n",
    "        for p in paragraphs:\n",
    "            article_text = p.text\n",
    "        \n",
    "        # Prepares text for analysis.\n",
    "        vocabulary = self.pre_process(article_text)\n",
    "        print(vocabulary)\n",
    "    \n",
    "    \n",
    "    def pre_process(self, text):\n",
    "        \"\"\"\n",
    "        Function: pre-processes text to prepare for analysis. \n",
    "        =====================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes <string> text to be pre-processed.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns <string> pre-processed text.\"\"\"\n",
    "        \n",
    "        # Cleaing the text\n",
    "        processed_article = text.lower()\n",
    "        processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "        processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "        \n",
    "        # Preparing the dataset\n",
    "        all_sentences = nltk.sent_tokenize(processed_article)\n",
    "        all_words = [nltk.word_tokenize(sentence) for sentence in all_sentences]\n",
    "        \n",
    "        # Removing Stop Words\n",
    "        for i in range(len(all_words)):\n",
    "            all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "        \n",
    "        word2vec = Word2Vec(all_words, min_count = 1)\n",
    "        vocabulary = word2vec.wv.vocab\n",
    "        return vocabulary\n",
    "        \n",
    "    \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_analysis(article_one, article_two):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       Right now this function takes two strings as its parameters (article_one, article_two). In the future, it should take \n",
    "       WikiArticle instances to allow multiple sub-headers to be analyzed together. \n",
    "       \n",
    "       Returns\n",
    "       --------\n",
    "       Jaccard Similarity Percentage.\"\"\"\n",
    "    \n",
    "    a = set(article_one.split(\" \"))\n",
    "    b = set(article_two.split(\" \"))\n",
    "    comparison = a.intersection(b)\n",
    "    return float(len(comparison)) / (len(a) + len(b) - len(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_over_threshold(similarity, **keyword_parameters):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       similarity (float): similarity value that will be checked against threshold.\n",
    "       threshold (float): Optional paramter to provide value for threshold. Must be passed as \"threshold = (value)\". Default is 50.\n",
    "    \n",
    "       Returns\n",
    "       ----------\n",
    "       Boolean value. True if threshold limit is met or exceeded, else False.\"\"\"\n",
    "    \n",
    "    if('threshold' in keyword_paramaters):\n",
    "        threshold = keyword_paramaters['threshold']\n",
    "    else:\n",
    "        threshold = 50\n",
    "    return (similarity >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main title similarity 0.25\n",
      "Secondary title similarity 0.3548387096774194\n"
     ]
    }
   ],
   "source": [
    "### Driver ###\n",
    "##          ##\n",
    "# ========== #\n",
    "\n",
    "\n",
    "article_one = WikiArticle(\"https://en.wikipedia.org/wiki/IBM_mainframe\")\n",
    "article_two = WikiArticle(\"https://en.wikipedia.org/wiki/History_of_IBM\")\n",
    "\n",
    "print(\"Main title similarity \" + str(jaccard_analysis(article_one.main_title, article_two.main_title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'software': <gensim.models.keyedvectors.Vocab object at 0x11b077748>, 'based': <gensim.models.keyedvectors.Vocab object at 0x11b0777f0>, 'emulators': <gensim.models.keyedvectors.Vocab object at 0x11b0778d0>, 'system': <gensim.models.keyedvectors.Vocab object at 0x11b077940>, 'z': <gensim.models.keyedvectors.Vocab object at 0x11b077978>, 'hardware': <gensim.models.keyedvectors.Vocab object at 0x11b0779e8>, 'including': <gensim.models.keyedvectors.Vocab object at 0x11b077a20>, 'flex': <gensim.models.keyedvectors.Vocab object at 0x11b077a90>, 'es': <gensim.models.keyedvectors.Vocab object at 0x11b077b00>, 'runs': <gensim.models.keyedvectors.Vocab object at 0x11b077c18>, 'unixware': <gensim.models.keyedvectors.Vocab object at 0x11b077cf8>, 'linux': <gensim.models.keyedvectors.Vocab object at 0x11b077e10>, 'freely': <gensim.models.keyedvectors.Vocab object at 0x11b077e80>, 'available': <gensim.models.keyedvectors.Vocab object at 0x11b077ef0>, 'hercules': <gensim.models.keyedvectors.Vocab object at 0x11b077fd0>, 'freebsd': <gensim.models.keyedvectors.Vocab object at 0x11b077f60>, 'solaris': <gensim.models.keyedvectors.Vocab object at 0x11b077f98>, 'macos': <gensim.models.keyedvectors.Vocab object at 0x11b077f28>, 'microsoft': <gensim.models.keyedvectors.Vocab object at 0x11b077668>, 'windows': <gensim.models.keyedvectors.Vocab object at 0x11b07b048>, 'ibm': <gensim.models.keyedvectors.Vocab object at 0x11b07b080>, 'offers': <gensim.models.keyedvectors.Vocab object at 0x11b07b0b8>, 'emulator': <gensim.models.keyedvectors.Vocab object at 0x11b07b0f0>, 'called': <gensim.models.keyedvectors.Vocab object at 0x11b07b128>, 'zpdt': <gensim.models.keyedvectors.Vocab object at 0x11b07b160>, 'personal': <gensim.models.keyedvectors.Vocab object at 0x11b07b198>, 'development': <gensim.models.keyedvectors.Vocab object at 0x11b07b1d0>, 'tool': <gensim.models.keyedvectors.Vocab object at 0x11b07b208>, 'x': <gensim.models.keyedvectors.Vocab object at 0x11b07b240>, 'machines': <gensim.models.keyedvectors.Vocab object at 0x11b07b278>}\n"
     ]
    }
   ],
   "source": [
    "### Scratch Work ###\n",
    "##                ##\n",
    "# ================ #\n",
    "\n",
    "article_one = WikiArticle(\"https://en.wikipedia.org/wiki/IBM_mainframe\")\n",
    "article_two = WikiArticle(\"https://en.wikipedia.org/wiki/History_of_IBM\")\n",
    "article_one.get_main_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
