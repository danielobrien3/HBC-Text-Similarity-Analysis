{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an attempt to make this notebook as organized (re: readable) as possible, I will explain its structure here. \n",
    "## The first cell is dedicated to initializing all dependencies used. \n",
    "## Each cell after is then dedicated to defining each function in the module\n",
    "### Finally, the last cell is this project's driver. That is where we will be putting the pieces of the puzzle together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports. 'Requests' for https requests. 'BeautifulSoup' for html scraping. 'Pandas' for data analysis. \n",
    "# 'sklearn' for similarity functions, such as word counter and cosine similarity. 'gensim' for Doc2Vec.\n",
    "# 'nltk' for pre-processing main text. 're' for regex. 'scipy' for spacial cosine. \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import doc2vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy import spatial\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import Counter\n",
    "\n",
    "# initializes training of doc2vec model. \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "# This can get initialized up here, as it will be constant throughout. \n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikiArticle class. Named 'wikiArticle' for lack of inspiration. Will hold all relevant data on an article. \n",
    "\n",
    "class WikiArticle:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = BeautifulSoup(requests.get(self.url).text, \"html\")\n",
    "        self.main_title = self.soup.find_all(\"h1\")[0].get_text()\n",
    "        self.secondary_titles = \"\"\n",
    "        self.main_text = \"\"\n",
    "        \n",
    "    def get_secondary_titles(self):\n",
    "        # Check length to make sure secondary_titles list hasn't already been filled. Don't want duplicate data messing us up. \n",
    "        if(len(self.secondary_titles) == 0):\n",
    "            for secondary_title in self.soup.find_all(\"h2\"):\n",
    "                self.secondary_titles += \" \" + secondary_title.get_text()\n",
    "                \n",
    "    def get_main_text(self):\n",
    "        \"\"\"\n",
    "        Function: self.main_text set to <string> pre-processed main text of article.\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no parameters.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns nothing.\"\"\"\n",
    "        \n",
    "        # Gets text from the article\n",
    "        paragraphs = self.soup.find_all(\"p\")\n",
    "        article_text = \"\"\n",
    "        for p in paragraphs:\n",
    "            article_text = article_text + \" \" + p.text\n",
    "        \n",
    "        # Prepares text for analysis.\n",
    "        self.main_text = self.pre_process(article_text)\n",
    "    \n",
    "    \n",
    "    def pre_process(self, text):\n",
    "        \"\"\"\n",
    "        Function: pre-processes text to prepare for analysis. \n",
    "        =====================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes <string> text to be pre-processed.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns <dict> Doc2Vec of pre-processed text.\"\"\"\n",
    "        \n",
    "        # Cleaing the text\n",
    "        processed_article = text.lower()\n",
    "        \n",
    "        # Preparing the dataset\n",
    "        all_words = word_tokenize(processed_article)\n",
    "        processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "        processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "        \n",
    "        # Removing Stop Words\n",
    "        processed_text = []\n",
    "        for w in all_words:\n",
    "            if not w in stopwords.words('english'):\n",
    "                processed_text.append(w) \n",
    "\n",
    "        return processed_text\n",
    "\n",
    "\n",
    "    \n",
    "    def get_related(self):\n",
    "        \"\"\" Function: Get list of related articles\n",
    "            =====================================================\n",
    "                Parameters\n",
    "                ----------\n",
    "                This function takes no paramater \n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                This function returns the array of wikiArticle objects from listed related articles. \n",
    "               \n",
    "               \"\"\"\n",
    "        related_list = self.soup.find(id=\"See_also\").parent.find_next('ul').findChildren('li')\n",
    "        \n",
    "        articles = []\n",
    "        for item in related_list:\n",
    "            link = item.findChild('a')\n",
    "            articles.append(WikiArticle(\"https://en.wikipedia.org\"+link.get('href')))\n",
    "        \n",
    "        self.related = articles\n",
    "        return self.related\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_analysis(article_one, article_two):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       Right now this function takes two strings as its parameters (article_one, article_two). In the future, it should take \n",
    "       WikiArticle instances to allow multiple sub-headers to be analyzed together. \n",
    "       \n",
    "       Returns\n",
    "       --------\n",
    "       Jaccard Similarity Percentage.\"\"\"\n",
    "    \n",
    "    a = set(article_one.split(\" \"))\n",
    "    b = set(article_two.split(\" \"))\n",
    "    comparison = a.intersection(b)\n",
    "    return float(len(comparison)) / (len(a) + len(b) - len(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_analysis(text_one, text_two):\n",
    "    # count word occurrences\n",
    "    article_one_vals = Counter(text_one)\n",
    "    article_two_vals = Counter(text_two)\n",
    "\n",
    "    # convert to word-vectors\n",
    "    words  = list(article_one_vals.keys() | article_two_vals.keys())\n",
    "    a_vect = [article_one_vals.get(word, 0) for word in words]        \n",
    "    b_vect = [article_two_vals.get(word, 0) for word in words]       \n",
    "\n",
    "    # find cosine\n",
    "    len_a  = sum(av*av for av in a_vect) ** 0.5             \n",
    "    len_b  = sum(bv*bv for bv in b_vect) ** 0.5             \n",
    "    dot    = sum(av*bv for av,bv in zip(a_vect, b_vect))    \n",
    "    cosine = dot / (len_a * len_b)\n",
    "    \n",
    "    # return cosine\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_over_threshold(similarity, *args):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       similarity (float): similarity value that will be checked against threshold.\n",
    "       threshold (float): Optional paramter to provide value for threshold. Must be passed as \"threshold = (value)\". Default is 50.\n",
    "    \n",
    "       Returns\n",
    "       ----------\n",
    "       Boolean value. True if threshold limit is met or exceeded, else False.\"\"\"\n",
    "    \n",
    "    if(len(args) == 1):\n",
    "        threshold = args[0]\n",
    "    else:\n",
    "        threshold = 50\n",
    "    return (similarity >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is over threshold.\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "### Driver ###\n",
    "##          ##\n",
    "# ========== #\n",
    "\n",
    "\n",
    "article_one = WikiArticle(\"https://en.wikipedia.org/wiki/IBM_mainframe\")\n",
    "article_two = WikiArticle(\"https://en.wikipedia.org/wiki/History_of_IBM\")\n",
    "\n",
    "# Check if main title similarity is over threshold\n",
    "if(is_over_threshold(jaccard_analysis(article_one.main_title, article_two.main_title), 0.10)):\n",
    "    print(\"is over threshold.\")\n",
    "else:\n",
    "    print(\"is not over threshold\")\n",
    "\n",
    "print(jaccard_analysis(article_one.main_title, article_two.main_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-cbead9aad1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0marticle_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_main_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0marticle_two\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_main_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_two\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-4d08ba92ac60>\u001b[0m in \u001b[0;36mcosine_analysis\u001b[0;34m(text_one, text_two)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# convert to word-vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mwords\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mb_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0ma_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marticle_one_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mb_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marticle_two_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a_vals' is not defined"
     ]
    }
   ],
   "source": [
    "### Scratch Work ###\n",
    "##                ##\n",
    "# ================ #\n",
    "\n",
    "article_one = WikiArticle(\"https://en.wikipedia.org/wiki/IBM_mainframe\")\n",
    "article_two = WikiArticle(\"https://en.wikipedia.org/wiki/History_of_IBM\")\n",
    "\n",
    "article_one.get_main_text()\n",
    "article_two.get_main_text()\n",
    "print(cosine_analysis(article_one.main_text, article_two.main_text))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
