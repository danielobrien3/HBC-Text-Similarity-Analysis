{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an attempt to make this notebook as organized (re: readable) as possible, I will explain its structure here. \n",
    "## The first cell is dedicated to initializing all dependencies used. \n",
    "## Each cell after is then dedicated to defining each function in the module\n",
    "### Finally, the last cell is this project's driver. That is where we will be putting the pieces of the puzzle together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "# Imports. 'Requests' for https requests. 'BeautifulSoup' for html scraping. 'Pandas' for data analysis. \n",
    "# 'sklearn' for similarity functions, such as word counter and cosine similarity. 'gensim' for Doc2Vec.\n",
    "# 'nltk' for pre-processing main text. 're' for regex. 'scipy' for spacial cosine. \n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import doc2vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from scipy import spatial\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "data_folder = Path(\"data/\")\n",
    "file = data_folder / \"GoogleNews-vectors-negative300.bin\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(file, binary=True, limit=500000)\n",
    "index2word_set = set(model.wv.index2word)\n",
    "\n",
    "# This can get initialized up here, as it will be constant throughout. \n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus class. This will hold all the article objects in our corpus and allow us to execute some corpus-wide methods.\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, main_article):\n",
    "        self.main_article = WikiArticle(main_article)\n",
    "        self.articles = []\n",
    "        self.corpus_stopwords = []\n",
    "        self.D2Vmodel = None\n",
    "        \n",
    "    def fill_corpus(self, mode):\n",
    "        \"\"\"Function: Fills corpus by getting related articles, starting with the main article and\n",
    "           using the other articles that are found until the corpus meets the set size parameter.  \n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Desired size of final corpus.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           No return. Corpus articles are stored in self.articles\"\"\"\n",
    "        if(mode == \"all_random\"):\n",
    "            size = 100;\n",
    "            while len(self.articles) < size:\n",
    "                art = WikiArticle(\"https://en.wikipedia.org/wiki/Special:Random\")\n",
    "                self.articles.append(art)\n",
    "                \n",
    "        if(mode == \"all_related\"):\n",
    "            size = 1000;\n",
    "            # Start filling corpus with artiles related to main article\n",
    "            self.articles = self.main_article.get_related()\n",
    "\n",
    "            # Keep track of current article using article counter\n",
    "            article_counter = 0\n",
    "\n",
    "            while len(self.articles) < size:\n",
    "                related_articles = self.articles[article_counter].get_related()\n",
    "                if(related_articles != False):\n",
    "                    self.articles.extend(self.articles[article_counter].get_related())\n",
    "                article_counter +=1\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Function: Analyzes all corpus against corpus main article and returns results in pandas dataframe.\n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           None \n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           No return. Builds and trains doc2vec model and stores it in corpus object\"\"\"\n",
    "        corpus = []\n",
    "        self.main_article.get_main_text()\n",
    "        corpus.append(gensim.models.doc2vec.TaggedDocument(words=self.main_article.main_text, tags=[0]))\n",
    "        tag_counter = 0\n",
    "        for article in self.articles:\n",
    "            article.get_main_text()\n",
    "            corpus.append(gensim.models.doc2vec.TaggedDocument(words=article.main_text, tags=[tag_counter]))\n",
    "            tag_counter += 1\n",
    "            \n",
    "        D2Vmodel = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=10)\n",
    "        D2Vmodel.build_vocab(corpus)\n",
    "        D2Vmodel.train(corpus, total_examples=D2Vmodel.corpus_count, epochs=20)\n",
    "        self.D2Vmodel = D2Vmodel            \n",
    "        \n",
    "    def similarity_analysis(self, smart):\n",
    "        \"\"\"Function: Analyzes all corpus against corpus main article and returns results in pandas dataframe.\n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           <boolean> Determines whether or not multi-tiered analysis is used. \n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns pandas dataframe with analysis results\"\"\"\n",
    "        \n",
    "        ## Pre-process main article\n",
    "        self.main_article.get_secondary_titles()\n",
    "        self.main_article.get_main_text()\n",
    "        self.main_article.get_word_frequency()\n",
    "        \n",
    "        ## Build and train model\n",
    "        self.build_model()\n",
    "        \n",
    "        ## Initialize Pandas Dataframe to store results\n",
    "        index = []\n",
    "        for article in self.articles:\n",
    "            if(article.main_title not in index):\n",
    "                index.append(article.main_title)\n",
    "\n",
    "        columns = [\"Main Title Tier\", \"Secondary Title Tier\", \"Main Analysis\"]\n",
    "        results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "        ## Actually do the analysis\n",
    "        ndx = 1 ## index for d2v tagging purposes\n",
    "        for article in self.articles:\n",
    "            results.loc[article.main_title] = self.main_article.similarity_analysis(article, smart, ndx, self.D2Vmodel)\n",
    "            ndx = ndx + 1\n",
    "        \n",
    "        return results\n",
    "            \n",
    "    \n",
    "    def new_article(self, url):\n",
    "        \"\"\"Function: add a specific wiki article to the corpus.\n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           (Wiki) Article url\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns index of new article.\"\"\"\n",
    "        self.articles.append(WikiArticle(url))\n",
    "        return len(self.articles) - 1\n",
    "    \n",
    "    def filter_corpus_by_frequency(self, *args):\n",
    "        \"\"\"Function: Finds a variable amount (default 3) of the most frequent words in the corpus. \n",
    "           These words are then removed from all of the article.word_frequency[] dictionaries. \n",
    "           ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           (Optional) <int> Number of stop words to get. Default is 3. \n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           No return. Filters most frequent words in corpus out of all articles word_frequency dictionaries.\"\"\"\n",
    "\n",
    "        \n",
    "        ## Lets loop through the articles in the corpus, get each articles word_frequency count, and merge them into a common dictionary.\n",
    "        # ...this will be fun\n",
    "        total_frequency = {}\n",
    "        for article in self.articles:\n",
    "            total_frequency = mergeDict(total_frequency, article.get_word_frequency())\n",
    "        total_frequency = Counter(total_frequency)\n",
    "        ## Okay now that's done, let's get the most frequently found words in the corpus (aka our new corpus stop words).\n",
    "        \n",
    "        # Check if optional paramater was passed. \n",
    "        if(len(args) == 1):\n",
    "            count = args[0]\n",
    "        else:\n",
    "            count = 3\n",
    "\n",
    "        # Get 3 most frequently found words and store them in corpus_stopwords list. \n",
    "        self.corpus_stopwords = [item[0] for item in total_frequency.most_common(count)]\n",
    "        \n",
    "        # Loop through all articles in corpus, filtering out the newly obtained corpus stop words.\n",
    "        for article in self.articles:\n",
    "            article.filter_corpus_stopwords(self.corpus_stopwords)\n",
    "        \n",
    "        \n",
    "# Class agnostic function to help merging word_frequency dicts\n",
    "def mergeDict(dict1, dict2):\n",
    "    ''' Merge dictionaries and keep values of common keys in list'''\n",
    "    dict3 = {**dict1, **dict2}\n",
    "    for key, value in dict3.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            dict3[key] = (value + dict1[key])\n",
    "    return Counter(dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikiArticle class. Named 'wikiArticle' for lack of inspiration. Will hold all relevant data on an article. \n",
    "\n",
    "class WikiArticle:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = BeautifulSoup(requests.get(self.url).text, \"html\")\n",
    "        self.main_title = self.soup.find_all(\"h1\")[0].get_text()\n",
    "        self.secondary_titles = \"\"\n",
    "        self.main_text = \"\"\n",
    "        self.word_frequency = {}\n",
    "        \n",
    "    \n",
    "    def similarity_analysis(self, article, smart, ndx, D2Vmodel):\n",
    "        \"\"\"\n",
    "        Function: To be used in Corpus class to perform analysis between main article and comparison article\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Article to be analyzed against main article.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns Pandas series to be used in forming a dataframe result.\"\"\"\n",
    "        ## Check main title tier\n",
    "        if(smart == True):\n",
    "            a_vec = self.w2v_vector(self.main_title);\n",
    "            b_vec = article.w2v_vector(article.main_title);\n",
    "            main_title_comparison = vec_cosine_analysis(a_vec, b_vec)\n",
    "            if is_over_threshold(main_title_comparison):\n",
    "                main_title_tier = True\n",
    "            else:\n",
    "                return pd.Series({'Main Title Tier': main_title_comparison, 'Secondary Title Tier': None, 'Main Analysis': None})\n",
    "\n",
    "            ## Check sub title tier\n",
    "            article.get_secondary_titles()\n",
    "            a_vec = self.w2v_vector(self.secondary_titles)\n",
    "            b_vec = article.w2v_vector(article.secondary_titles)\n",
    "            secondary_title_comparison = vec_cosine_analysis(a_vec, b_vec)\n",
    "            if is_over_threshold(secondary_title_comparison):\n",
    "                secondary_title_tier = True\n",
    "            else:\n",
    "                return pd.Series({'Main Title Tier': main_title_comparison, 'Secondary Title Tier': secondary_title_comparison, 'Main Analysis': None})\n",
    "        else: \n",
    "            main_title_comparison = None;\n",
    "            secondary_title_comparison = None;\n",
    "        \n",
    "        ## Since we got this far we should get the main text and wrod count of the article\n",
    "        article.get_main_text()\n",
    "        article.get_word_frequency()\n",
    "        \n",
    "        ## And now perform the main analysis \n",
    "        a_vec = D2Vmodel.infer_vector(self.main_text)\n",
    "        b_vec = D2Vmodel.infer_vector(article.main_text)\n",
    "        analysis = vec_cosine_analysis(a_vec, b_vec)\n",
    "        return pd.Series({'Main Title Tier': main_title_comparison, 'Secondary Title Tier': secondary_title_comparison, 'Main Analysis':analysis})\n",
    "        \n",
    "            \n",
    "    \n",
    "    def get_secondary_titles(self):\n",
    "        # Check length to make sure secondary_titles list hasn't already been filled. Don't want duplicate data messing us up. \n",
    "        if(len(self.secondary_titles) == 0):\n",
    "            for secondary_title in self.soup.find_all(\"h2\"):\n",
    "                self.secondary_titles += \" \" + secondary_title.get_text()\n",
    "    \n",
    "    def w2v_vector(self, text):\n",
    "        \"\"\"\n",
    "        Function: To get word2vec average vector of provided text\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Text to be word2vec'd\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Word2Vec average vector of provided text.\"\"\"\n",
    "        num_features = 300;\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "\n",
    "        for word in text:\n",
    "            if word in index2word_set:\n",
    "                nwords = nwords+1\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "        if nwords>0:\n",
    "            featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec\n",
    "        \n",
    "        \n",
    "                \n",
    "    def get_main_text(self):\n",
    "        \"\"\"\n",
    "        Function: self.main_text set to <string> pre-processed main text of article.\n",
    "        ============================================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no parameters.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns nothing.\"\"\"\n",
    "        \n",
    "        # Gets text from the article\n",
    "        paragraphs = self.soup.find_all(\"p\")\n",
    "        article_text = \"\"\n",
    "        for p in paragraphs:\n",
    "            article_text = article_text + \" \" + p.text\n",
    "        \n",
    "        # Prepares text for analysis.\n",
    "        self.main_text = self.pre_process(article_text)\n",
    "    \n",
    "    def get_word_frequency(self):\n",
    "        \"\"\" \n",
    "        Function: gets word frequencies for article and stores in self.word_frequency dictionary. \n",
    "        ===========================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes no paramaters\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns word frequency. Result is stored in self.word_frequency\"\"\"\n",
    "        \n",
    "        self.word_frequency = Counter(self.main_text)\n",
    "        return self.word_frequency\n",
    "        \n",
    "        \n",
    "    def filter_corpus_stopwords(self, corpus_stop_words):\n",
    "        \"\"\" \n",
    "        Function: removes all occurences of the most frequent words in the corpus from self.word_frequency. This function should only be called from within a corpus class method.\n",
    "        ===========================================\n",
    "        Parameters\n",
    "        ----------\n",
    "        <list> corpus stop words\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        No return. self.word_frequency\"\"\"\n",
    "        \n",
    "        filtered_text = Counter({})\n",
    "        for k, v in self.word_frequency.items():\n",
    "            if not k in corpus_stop_words:\n",
    "                filtered_text[k] = v\n",
    "                \n",
    "        self.word_frequency = filtered_text\n",
    "    \n",
    "    \n",
    "    def pre_process(self, text):\n",
    "        \"\"\"\n",
    "        Function: pre-processes text to prepare for analysis. \n",
    "        =====================================================\n",
    "           Parameters\n",
    "           ----------\n",
    "           Takes <string> text to be pre-processed.\n",
    "\n",
    "           Returns\n",
    "           ----------\n",
    "           Returns <dict> Doc2Vec of pre-processed text.\"\"\"\n",
    "        \n",
    "        # Cleaing the text\n",
    "        processed_article = text.lower()\n",
    "        \n",
    "        # Preparing the dataset\n",
    "        all_words = word_tokenize(processed_article)\n",
    "        processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "        processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "        \n",
    "        # Removing Stop Words\n",
    "        processed_text = []\n",
    "        for w in all_words:\n",
    "            if not w in stopwords.words('english') and not w in string.punctuation:\n",
    "                processed_text.append(w)\n",
    "        \n",
    "        return processed_text\n",
    "\n",
    "    \n",
    "    def get_related(self):\n",
    "        \"\"\" Function: Get list of related articles\n",
    "            =====================================================\n",
    "                Parameters\n",
    "                ----------\n",
    "                This function takes no paramater \n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                If related articles exist, they are returned in a list. Else returns false. \n",
    "               \n",
    "               \"\"\"\n",
    "        if(self.soup.find(id=\"See_also\") is None):\n",
    "            return False\n",
    "        else:\n",
    "            related_list = self.soup.find(id=\"See_also\").parent.find_next('ul').findChildren('li')\n",
    "\n",
    "            articles = []\n",
    "            for item in related_list:\n",
    "                link = item.findChild('a')\n",
    "                articles.append(WikiArticle(\"https://en.wikipedia.org\"+link.get('href')))\n",
    "\n",
    "            return articles\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_analysis(article_one, article_two):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       Right now this function takes two strings as its parameters (article_one, article_two). In the future, it should take \n",
    "       WikiArticle instances to allow multiple sub-headers to be analyzed together. \n",
    "       \n",
    "       Returns\n",
    "       --------\n",
    "       Jaccard Similarity Percentage.\"\"\"\n",
    "    \n",
    "    a = set(article_one.split(\" \"))\n",
    "    b = set(article_two.split(\" \"))\n",
    "    comparison = a.intersection(b)\n",
    "    return float(len(comparison)) / (len(a) + len(b) - len(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_analysis(article_one_frequency, article_two_frequency):\n",
    "\n",
    "    # convert to word-vectors\n",
    "    words  = list(article_one_frequency.keys() | article_two_frequency.keys())\n",
    "    a_vect = [article_one_frequency.get(word, 0) for word in words]        \n",
    "    b_vect = [article_two_frequency.get(word, 0) for word in words]       \n",
    "\n",
    "    # find cosine\n",
    "    len_a  = sum(av*av for av in a_vect) ** 0.5             \n",
    "    len_b  = sum(bv*bv for bv in b_vect) ** 0.5             \n",
    "    dot    = sum(av*bv for av,bv in zip(a_vect, b_vect))    \n",
    "    cosine = dot / (len_a * len_b)\n",
    "    \n",
    "    # return cosine\n",
    "    return cosine\n",
    "\n",
    "def vec_cosine_analysis(a_vect, b_vect):\n",
    "    # find cosine\n",
    "    len_a  = sum(av*av for av in a_vect) ** 0.5             \n",
    "    len_b  = sum(bv*bv for bv in b_vect) ** 0.5             \n",
    "    dot    = sum(av*bv for av,bv in zip(a_vect, b_vect))    \n",
    "    cosine = dot / (len_a * len_b)\n",
    "    \n",
    "    # return cosine\n",
    "    return cosine\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_over_threshold(similarity, *args):\n",
    "    \n",
    "    \"\"\"Parameters\n",
    "       ----------\n",
    "       similarity (float): similarity value that will be checked against threshold.\n",
    "       threshold (float): Optional paramater to provide value for threshold. Must be passed as \"threshold = (value)\". Default is 50.\n",
    "    \n",
    "       Returns\n",
    "       ----------\n",
    "       Boolean value. True if threshold limit is met or exceeded, else False.\"\"\"\n",
    "    \n",
    "    if(len(args) == 1):\n",
    "        threshold = args[0]\n",
    "    else:\n",
    "        threshold = .5\n",
    "    return (similarity >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Driver ###\n",
    "##          ##\n",
    "# ========== #/\n",
    "\n",
    "# Initiate corpus with article of focus\n",
    "corpus = Corpus(\"https://en.wikipedia.org/wiki/IBM_mainframe\")\n",
    "corpus.fill_corpus(\"all_related\")\n",
    "corpus.filter_corpus_by_frequency()\n",
    "\n",
    "corpus.similarity_analysis(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scratch Work ###\n",
    "##                ##\n",
    "# ================ #\n",
    "\n",
    "\n",
    "print(corpus.get_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
